{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3c01442",
   "metadata": {},
   "source": [
    "# 03: M1/S1 - Unsupervised Anomaly Detection (Token Level)\n",
    "\n",
    "**Goal:** Detect subjective words (LJMPNIK) using unsupervised methods (Mahalanobis, Isolation Forest, OCSVM).\n",
    " \n",
    "**Methodology:**\n",
    "1. **Training:** On purely neutral tokens (L0) from `gold` dataset.\n",
    "2. **Validation:** On mixed data (L0 + L1) to find optimal threshold.\n",
    "3. **Testing:** On held-out mixed data (Document-level split).\n",
    " \n",
    "**Note:** Uses new `data_splitting` module to prevent data leakage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e8d57f",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f6968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "‚úÖ Setup complete. Data dir: C:\\Users\\dobes\\Documents\\UniversityCodingProject\\ThesisCoding\\data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from itables import show\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Auto-reload modules for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Add src to path\n",
    "current_dir = os.getcwd()\n",
    "src_dir = os.path.abspath(os.path.join(current_dir, '..', 'src'))\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)\n",
    "\n",
    "# Import custom modules\n",
    "import config\n",
    "import data_splitting\n",
    "import models\n",
    "import visualization\n",
    "import experiments\n",
    "import evaluation\n",
    "\n",
    "\n",
    "# Setup visualization style\n",
    "visualization.setup_style()\n",
    "\n",
    "print(f\"‚úÖ Setup complete. Data dir: {config.DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf24556",
   "metadata": {},
   "source": [
    "## 2. Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c76a64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 19:35:20,970 - INFO - üìä Preparing scenario: baseline (token level, aggressive filter)\n",
      "2026-02-03 19:35:21,557 - INFO - ‚úÖ Loaded 17557 rows from C:\\Users\\dobes\\Documents\\UniversityCodingProject\\ThesisCoding\\data\\processed\\gold_tokens.pkl\n",
      "2026-02-03 19:35:27,085 - INFO - ‚úÖ Loaded 78991 rows from C:\\Users\\dobes\\Documents\\UniversityCodingProject\\ThesisCoding\\data\\processed\\silver_tokens.pkl\n",
      "2026-02-03 19:35:27,292 - INFO - Splitting 520 documents: 104 test, 41 val, 375 train\n",
      "2026-02-03 19:35:27,331 - INFO - ‚úÖ Document-level split completed:\n",
      "2026-02-03 19:35:27,334 - INFO -    Train: 376 docs, 2585 samples\n",
      "2026-02-03 19:35:27,337 - INFO -    Val:   41 docs, 270 samples\n",
      "2026-02-03 19:35:27,340 - INFO -    Test:  103 docs, 741 samples\n",
      "2026-02-03 19:35:27,344 - INFO -    ‚úì No document leakage detected between splits\n",
      "2026-02-03 19:35:27,347 - INFO - ‚úÖ Scenario data prepared:\n",
      "2026-02-03 19:35:27,352 - INFO -    Train: 2585 samples (L0: 900, L1: 1685)\n",
      "2026-02-03 19:35:27,356 - INFO -    Val:   270 samples (L0: 96, L1: 174)\n",
      "2026-02-03 19:35:27,359 - INFO -    Test:  741 samples (L0: 256, L1: 485)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ TRAIN set (Neutral only): (2585, 768)\n",
      "üîπ VAL set (Mixed):          (270, 768)\n",
      "üîπ TEST set (Mixed):         (741, 768)\n",
      "‚ö†Ô∏è Anomalies in Train: 176 (Should be 0)\n"
     ]
    }
   ],
   "source": [
    "# Load sample data (Aggressive filter) just to check stats\n",
    "\n",
    "data_sample = data_splitting.get_train_val_test_splits(\n",
    "    scenario='baseline', \n",
    "    level='token', \n",
    "    filter_type='aggressive'\n",
    ")\n",
    "\n",
    "print(f\"üîπ TRAIN set (Neutral only): {data_sample['X_train'].shape}\")\n",
    "print(f\"üîπ VAL set (Mixed):          {data_sample['X_val'].shape}\")\n",
    "print(f\"üîπ TEST set (Mixed):         {data_sample['X_test'].shape}\")\n",
    "\n",
    "# Verify Train contains only L0\n",
    "train_anomalies = data_sample['y_train'].sum()\n",
    "print(f\"‚ö†Ô∏è Anomalies in Train: {train_anomalies} (Should be 0)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff57deea",
   "metadata": {},
   "source": [
    "## 3. Experimental Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d5eef3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Defined 9 scenarios.\n"
     ]
    }
   ],
   "source": [
    "# Define scenarios to run\n",
    "\n",
    "scenarios = []\n",
    "filters = ['aggressive', 'mild', 'none']\n",
    "models_list = ['Mahalanobis', 'IsolationForest', 'OCSVM']\n",
    "\n",
    "for m in models_list:\n",
    "    for f in filters:\n",
    "        scenarios.append({\n",
    "            'model': m,\n",
    "            'filter': f,\n",
    "            'level': 'token' # This notebook is S1 (Token)\n",
    "        })\n",
    "\n",
    "print(f\"üöÄ Defined {len(scenarios)} scenarios.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876f3e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN EXPERIMENTS (This uses the new src.experiments module)\n",
    "\n",
    "df_results = experiments.run_unsupervised_benchmark(scenarios)\n",
    "\n",
    "# Save results\n",
    "save_path = config.RESULTS_DIR / 'M1_S1_results.csv'\n",
    "df_results.to_csv(save_path, index=False)\n",
    "print(f\"üíæ Results saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51bf6fe",
   "metadata": {},
   "source": [
    "## 4. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e84b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show interactive table\n",
    "show(df_results.sort_values('auprc', ascending=False), classes=\"display compact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d325a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot of AUPRC Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df_results, x='model', y='auprc', hue='filter', palette='viridis')\n",
    "plt.title('M1/S1 Performance (AUPRC) by Model and Filter')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(title='POS Filter', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaa5422",
   "metadata": {},
   "source": [
    "## 5. Deep Dive: Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97953bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get Winner\n",
    "best_run = df_results.sort_values('auprc', ascending=False).iloc[0]\n",
    "print(f\"üèÜ WINNER: {best_run['model']} ({best_run['filter']})\")\n",
    "print(f\"   AUPRC: {best_run['auprc']:.4f}\")\n",
    "print(f\"   F1:    {best_run['f1']:.4f}\")\n",
    "\n",
    "# 2. Reload Data for Winner\n",
    "data_best = data_splitting.get_train_val_test_splits(\n",
    "    scenario='baseline', \n",
    "    level='token', \n",
    "    filter_type=best_run['filter']\n",
    ")\n",
    "\n",
    "# 3. Retrain (to get the object)\n",
    "model = models.get_unsupervised_model(best_run['model'])\n",
    "model.fit(data_best['X_train'][data_best['y_train'] == 0])\n",
    "\n",
    "# 4. Get Scores\n",
    "scores_val = model.decision_function(data_best['X_val'])\n",
    "scores_test = model.decision_function(data_best['X_test'])\n",
    "\n",
    "# 5. Optimal Threshold (from Val)\n",
    "threshold, _ = evaluation.find_optimal_threshold(data_best['y_val'], scores_val, metric='f1')\n",
    "print(f\"‚öôÔ∏è Optimal Threshold (from Val): {threshold:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df9578a",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88705335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Anomaly Score Histogram\n",
    "visualization.plot_anomaly_histogram(\n",
    "    scores_test, \n",
    "    data_best['y_test'], \n",
    "    threshold=threshold,\n",
    "    title=f\"Anomaly Scores: {best_run['model']} ({best_run['filter']})\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2934a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Precision-Recall Curve\n",
    "visualization.plot_pr_curve(\n",
    "    data_best['y_test'], \n",
    "    scores_test, \n",
    "    title=f\"PR Curve: {best_run['model']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c195205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Confusion Matrix\n",
    "y_pred_test = (scores_test > threshold).astype(int)\n",
    "visualization.plot_confusion_matrix_heatmap(\n",
    "    data_best['y_test'], \n",
    "    y_pred_test, \n",
    "    normalize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1158d7cf",
   "metadata": {},
   "source": [
    "## 6. Qualitative Analysis (Export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad8d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "df_qual = pd.DataFrame({\n",
    "    'token_text': data_best['meta_test']['analyzed_token'],\n",
    "    'full_text': data_best['meta_test']['text'],\n",
    "    'true_label': data_best['y_test'],\n",
    "    'pred_label': y_pred_test,\n",
    "    'anomaly_score': scores_test,\n",
    "    'document_id': data_best['meta_test']['document_id']\n",
    "})\n",
    "\n",
    "# Add Error Category\n",
    "conditions = [\n",
    "    (df_qual.true_label == 1) & (df_qual.pred_label == 1),\n",
    "    (df_qual.true_label == 0) & (df_qual.pred_label == 0),\n",
    "    (df_qual.true_label == 0) & (df_qual.pred_label == 1),\n",
    "    (df_qual.true_label == 1) & (df_qual.pred_label == 0)\n",
    "]\n",
    "df_qual['category'] = np.select(conditions, ['TP', 'TN', 'FP', 'FN'])\n",
    "\n",
    "# Save\n",
    "qual_path = config.RESULTS_DIR / f\"M1_S1_qualitative_{best_run['model']}.csv\"\n",
    "df_qual.to_csv(qual_path, index=False)\n",
    "print(f\"üìù Qualitative analysis saved to: {qual_path}\")\n",
    "\n",
    "# Show top False Positives (High score, but neutral)\n",
    "print(\"\\n‚ùå Top False Positives (Model thought it was anomaly, but it's not):\")\n",
    "show(df_qual[df_qual.category == 'FP'].sort_values('anomaly_score', ascending=False).head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
