{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# 06: M2/S2 - Supervised Classification (Sentence Level)\n",
    "\n",
    "**C√≠l:** Natr√©novat klasifik√°tory na √∫rovni cel√Ωch vƒõt.\n",
    "**Hypot√©za:** Vƒõta nese v√≠ce kontextu ne≈æ samotn√© slovo. Funguje l√©pe pr≈Ømƒõr v≈°ech token≈Ø (**Mean Pooling**) nebo speci√°ln√≠ token (**[CLS]**)?\n",
    "\n",
    "**Sc√©n√°≈ôe:**\n",
    "* **S2a - Gold Balanced:** Tr√©nink na Gold datech (undersampling L0 na 1:1).\n",
    "* **S2b - Hybrid:** Tr√©nink na mixu Gold (L0) + Silver (L1).\n",
    "\n",
    "**Pooling Metody:**\n",
    "* **Mean:** Pr≈Ømƒõr embedding≈Ø v≈°ech slov ve vƒõtƒõ.\n",
    "* **CLS:** Embedding speci√°ln√≠ho tokenu [CLS] (reprezentace cel√© sekvence dle BERTa)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configuration loaded. Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 15:21:26,534 - INFO - üé® Visualization style set: whitegrid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete. Results dir: C:\\Users\\dobes\\Documents\\UniversityCodingProject\\ThesisCoding\\results\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import os\n",
    "from itables import show\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Auto-reload modules for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Add src to path\n",
    "current_dir = os.getcwd()\n",
    "src_dir = os.path.abspath(os.path.join(current_dir, '..', 'src'))\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)\n",
    "\n",
    "\n",
    "# Vlastn√≠ moduly\n",
    "import config\n",
    "import data_splitting\n",
    "import models\n",
    "import evaluation\n",
    "import visualization\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Nastaven√≠ vizualizace\n",
    "visualization.setup_style()\n",
    "\n",
    "print(f\"‚úÖ Setup complete. Results dir: {config.RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "datacheck",
   "metadata": {},
   "source": [
    "## 2. Data Check\n",
    "Ovƒõ≈ô√≠me poƒçty vƒõt pro jednotliv√© sc√©n√°≈ôe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "check_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 15:21:45,187 - INFO - üìä Preparing scenario: baseline (sentence level, aggressive filter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä DATA CHECK REPORT (M2/S2 - Sentence Level)\n",
      "================================================================================\n",
      "\n",
      "üîπ SC√âN√Å≈ò: BASELINE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 15:21:45,386 - INFO - ‚úÖ Loaded 1560 rows from C:\\Users\\dobes\\Documents\\UniversityCodingProject\\ThesisCoding\\data\\processed\\gold_sentences.pkl\n",
      "2026-02-05 15:21:46,041 - INFO - ‚úÖ Loaded 5709 rows from C:\\Users\\dobes\\Documents\\UniversityCodingProject\\ThesisCoding\\data\\processed\\silver_sentences.pkl\n",
      "2026-02-05 15:21:46,081 - INFO - Splitting 520 documents: 104 test, 41 val, 375 train\n",
      "2026-02-05 15:21:46,095 - INFO - ‚úÖ Document-level split completed:\n",
      "2026-02-05 15:21:46,099 - INFO -    Train: 376 docs, 376 samples\n",
      "2026-02-05 15:21:46,099 - INFO -    Val:   41 docs, 41 samples\n",
      "2026-02-05 15:21:46,101 - INFO -    Test:  103 docs, 103 samples\n",
      "2026-02-05 15:21:46,103 - INFO -    ‚úì No document leakage detected between splits\n",
      "2026-02-05 15:21:46,107 - INFO - ‚úÖ Scenario data prepared:\n",
      "2026-02-05 15:21:46,107 - INFO -    Train: 376 samples (L0: 136, L1: 240)\n",
      "2026-02-05 15:21:46,107 - INFO -    Val:   41 samples (L0: 15, L1: 26)\n",
      "2026-02-05 15:21:46,114 - INFO -    Test:  103 samples (L0: 37, L1: 66)\n",
      "2026-02-05 15:21:46,152 - INFO - üìä Preparing scenario: hybrid (sentence level, aggressive filter)\n",
      "2026-02-05 15:21:46,300 - INFO - ‚úÖ Loaded 1560 rows from C:\\Users\\dobes\\Documents\\UniversityCodingProject\\ThesisCoding\\data\\processed\\gold_sentences.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TRAIN  | Total: 376   | L0: 136  | L1: 240  | Ratio: 0.6:1\n",
      "   VAL    | Total: 41    | L0: 15   | L1: 26   | Ratio: 0.6:1\n",
      "   TEST   | Total: 103   | L0: 37   | L1: 66   | Ratio: 0.6:1\n",
      "\n",
      "üîπ SC√âN√Å≈ò: HYBRID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 15:21:46,830 - INFO - ‚úÖ Loaded 5709 rows from C:\\Users\\dobes\\Documents\\UniversityCodingProject\\ThesisCoding\\data\\processed\\silver_sentences.pkl\n",
      "2026-02-05 15:21:46,847 - INFO - Splitting 1472 documents: 294 test, 117 val, 1061 train\n",
      "2026-02-05 15:21:46,848 - INFO - ‚úÖ Document-level split completed:\n",
      "2026-02-05 15:21:46,848 - INFO -    Train: 1062 docs, 1062 samples\n",
      "2026-02-05 15:21:46,848 - INFO -    Val:   117 docs, 117 samples\n",
      "2026-02-05 15:21:46,862 - INFO -    Test:  293 docs, 293 samples\n",
      "2026-02-05 15:21:46,867 - INFO -    ‚úì No document leakage detected between splits\n",
      "2026-02-05 15:21:46,867 - INFO -    Balanced via undersampling: 136 + 926 ‚Üí 272\n",
      "2026-02-05 15:21:46,867 - INFO - ‚úÖ Scenario data prepared:\n",
      "2026-02-05 15:21:46,878 - INFO -    Train: 272 samples (L0: 136, L1: 136)\n",
      "2026-02-05 15:21:46,882 - INFO -    Val:   117 samples (L0: 15, L1: 102)\n",
      "2026-02-05 15:21:46,884 - INFO -    Test:  293 samples (L0: 37, L1: 256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TRAIN  | Total: 272   | L0: 136  | L1: 136  | Ratio: 1.0:1\n",
      "   VAL    | Total: 117   | L0: 15   | L1: 102  | Ratio: 0.1:1\n",
      "   TEST   | Total: 293   | L0: 37   | L1: 256  | Ratio: 0.1:1\n"
     ]
    }
   ],
   "source": [
    "SCENARIOS_TO_CHECK = ['baseline', 'hybrid']\n",
    "POOLING = 'mean' # Poƒçty jsou stejn√© pro mean i cls, li≈°√≠ se jen dimenze X\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"üìä DATA CHECK REPORT (M2/S2 - Sentence Level)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for scenario in SCENARIOS_TO_CHECK:\n",
    "    print(f\"\\nüîπ SC√âN√Å≈ò: {scenario.upper()}\")\n",
    "    try:\n",
    "        data = data_splitting.get_train_val_test_splits(\n",
    "            scenario=scenario,\n",
    "            level='sentence',\n",
    "            pooling=POOLING,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        def print_stats(name, y):\n",
    "            n_l0, n_l1 = np.sum(y == 0), np.sum(y == 1)\n",
    "            ratio = n_l0 / n_l1 if n_l1 > 0 else 0\n",
    "            print(f\"   {name:<6} | Total: {len(y):<5} | L0: {n_l0:<4} | L1: {n_l1:<4} | Ratio: {ratio:.1f}:1\")\n",
    "\n",
    "        print_stats(\"TRAIN\", data['y_train'])\n",
    "        print_stats(\"VAL\",   data['y_val'])\n",
    "        print_stats(\"TEST\",  data['y_test'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Chyba: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiments",
   "metadata": {},
   "source": [
    "## 3. Experiment Loop (Pooling & Scenarios)\n",
    "Tr√©nujeme kombinace: **Sc√©n√°≈ô x Pooling x Model**.\n",
    "\n",
    "**Sc√©n√°≈ôe:**\n",
    "* **S2a:** Baseline (Gold) + Manual Undersampling\n",
    "* **S2b:** Hybrid (Gold L0 + Silver L1)\n",
    "\n",
    "**Pooling:** `mean` vs `cls`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a8ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cesta pro v√Ωsledky\n",
    "RESULTS_PATH = config.RESULTS_DIR / \"M2_S2_experiment_results_v1.csv\"\n",
    "\n",
    "# Definice experiment≈Ø (Sc√©n√°≈ôe)\n",
    "SCENARIOS = [\n",
    "    {'id': 'S2a', 'name': 'Gold Balanced', 'scenario': 'baseline', 'balance_train': True},\n",
    "    {'id': 'S2b', 'name': 'Hybrid (G+S)',  'scenario': 'hybrid',   'balance_train': False}, # Hybrid je u≈æ balanced z modulu\n",
    "]\n",
    "\n",
    "POOLING_METHODS = ['mean', 'cls']\n",
    "\n",
    "MODELS_TO_TEST = ['LogReg', 'SVM (RBF)'] # Pro rychlost. SVM (Lin) m≈Ø≈æe≈° p≈ôidat.\n",
    "# MODELS_TO_TEST = [\"LogReg\", \"SVM (RBF)\", \"XGBoost\", \"Dummy\", \"SVM (Lin)\", \"NaiveBayes\", \"RandForest\"]\n",
    "\n",
    "if models.XGBOOST_AVAILABLE:\n",
    "    MODELS_TO_TEST.append('XGBoost')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_experiments",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "best_f1 = 0.0\n",
    "best_run = None\n",
    "\n",
    "print(f\"üöÄ STARTING SENTENCE LEVEL EXPERIMENTS...\")\n",
    "print(f\"üíæ Results path: {RESULTS_PATH}\")\n",
    "\n",
    "for pooling in POOLING_METHODS:\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"üåä POOLING METHOD: {pooling.upper()}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    \n",
    "    for exp in SCENARIOS:\n",
    "        print(f\"\\n   üß™ SCENARIO: {exp['id']} - {exp['name']}\")\n",
    "        \n",
    "        # 1. Naƒçten√≠ dat (se spr√°vn√Ωm poolingem)\n",
    "        try:\n",
    "            data = data_splitting.get_train_val_test_splits(\n",
    "                scenario=exp['scenario'],\n",
    "                level='sentence',\n",
    "                pooling=pooling,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            X_train, y_train = data['X_train'], data['y_train']\n",
    "            X_val, y_val     = data['X_val'], data['y_val']\n",
    "            X_test, y_test   = data['X_test'], data['y_test']\n",
    "            \n",
    "            # 2. Manu√°ln√≠ Undersampling pro S2a (Baseline)\n",
    "            if exp['balance_train'] and exp['scenario'] == 'baseline':\n",
    "                idx_l0 = np.where(y_train == 0)[0]\n",
    "                idx_l1 = np.where(y_train == 1)[0]\n",
    "                \n",
    "                np.random.seed(42)\n",
    "                idx_l0_down = np.random.choice(idx_l0, size=len(idx_l1), replace=False)\n",
    "                idx_balanced = np.concatenate([idx_l0_down, idx_l1])\n",
    "                np.random.shuffle(idx_balanced)\n",
    "                \n",
    "                X_train, y_train = X_train[idx_balanced], y_train[idx_balanced]\n",
    "                print(f\"      ‚öñÔ∏è Balanced Train Size: {X_train.shape[0]} (L1: {sum(y_train)})\")\n",
    "\n",
    "            # 3. Tr√©nink Model≈Ø\n",
    "            for model_name in MODELS_TO_TEST:\n",
    "                print(f\"      ‚öôÔ∏è {model_name}...\")\n",
    "                \n",
    "                try:\n",
    "                    clf = models.get_supervised_model(model_name, random_state=42)\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    \n",
    "                    # Sk√≥re\n",
    "                    if hasattr(clf, \"predict_proba\"):\n",
    "                        s_train = clf.predict_proba(X_train)[:, 1]\n",
    "                        s_val   = clf.predict_proba(X_val)[:, 1]\n",
    "                        s_test  = clf.predict_proba(X_test)[:, 1]\n",
    "                    else:\n",
    "                        s_train = clf.decision_function(X_train)\n",
    "                        s_val   = clf.decision_function(X_val)\n",
    "                        s_test  = clf.decision_function(X_test)\n",
    "                    \n",
    "                    # Threshold z Val\n",
    "                    threshold, _ = evaluation.find_optimal_threshold(y_val, s_val, metric='f1')\n",
    "                    \n",
    "                    # Metriky\n",
    "                    metrics = evaluation.calculate_metrics(y_test, (s_test > threshold).astype(int), s_test)\n",
    "                    \n",
    "                    # Log\n",
    "                    res = {\n",
    "                        'id': exp['id'],\n",
    "                        'scenario': exp['scenario'],\n",
    "                        'scenario_name': exp['name'],\n",
    "                        'pooling': pooling,\n",
    "                        'model': model_name,\n",
    "                        'balance_train': exp['balance_train'],\n",
    "                        'threshold': threshold,\n",
    "                        'test_f1': metrics['f1'],\n",
    "                        'test_auprc': metrics['avg_precision'],\n",
    "                        'test_roc_auc': metrics['roc_auc']\n",
    "                    }\n",
    "                    results.append(res)\n",
    "                    pd.DataFrame(results).to_csv(RESULTS_PATH, index=False)\n",
    "                    \n",
    "                    # Best Run Check\n",
    "                    if metrics['f1'] > best_f1:\n",
    "                        best_f1 = metrics['f1']\n",
    "                        best_run = {\n",
    "                            'info': res,\n",
    "                            'model': clf,\n",
    "                            'data': data,\n",
    "                            'scores_test': s_test,\n",
    "                            'y_test': y_test\n",
    "                        }\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"      ‚ùå Error {model_name}: {e}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error loading data: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ All experiments finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results",
   "metadata": {},
   "source": [
    "## 4. Results Overview\n",
    "Srovn√°n√≠ vlivu Poolingu a Sc√©n√°≈ô≈Ø."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS_PATH = config.RESULTS_DIR / \"M2_S2_experiment_results_v1.csv\"         x mo≈æno upravit pro naƒçten√≠ jin√©ho csv souboru\n",
    "\n",
    "# Naƒçten√≠ v√Ωsledk≈Ø\n",
    "if RESULTS_PATH.exists():\n",
    "    df_results = pd.read_csv(RESULTS_PATH)\n",
    "else:\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "# 1. Tabulka\n",
    "print(\"üìä SROVN√ÅN√ç F1 SK√ìRE (Pooling x Scenario):\")\n",
    "pivot = df_results.pivot_table(\n",
    "    values='test_f1', \n",
    "    index=['id', 'scenario_name'], \n",
    "    columns=['pooling', 'model'], \n",
    "    aggfunc='max'\n",
    ")\n",
    "display(pivot.style.background_gradient(cmap='Greens', axis=None).format(\"{:.4f}\"))\n",
    "\n",
    "# 2. Graf: Vliv Poolingu (Mean vs CLS)\n",
    "def plot_pooling_comparison(df, metric='f1'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Vytvo≈ô√≠me popisek modelu i se sc√©n√°≈ôem\n",
    "    df = df.copy()\n",
    "    df['Model_Exp'] = df['id'] + \": \" + df['model']\n",
    "    \n",
    "    sns.barplot(\n",
    "        data=df,\n",
    "        x='Model_Exp',\n",
    "        y=f'test_{metric}',\n",
    "        hue='pooling',\n",
    "        palette={'mean': config.COLORS['l0'], 'cls': config.COLORS['l1']}, # Mean=Modr√°, CLS=ƒåerven√°\n",
    "        edgecolor='white'\n",
    "    )\n",
    "    \n",
    "    plt.title(f\"Pooling Comparison: Mean vs CLS ({metric.upper()})\", fontsize=15, pad=15)\n",
    "    plt.ylabel(f\"Test {metric.upper()}\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "    plt.legend(title=\"Pooling\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nüìä GRAF: Pooling Comparison\")\n",
    "plot_pooling_comparison(df_results, metric='f1')\n",
    "plot_pooling_comparison(df_results, metric='auprc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deep_dive",
   "metadata": {},
   "source": [
    "## 5. Deep Dive: Winner Analysis\n",
    "Detailn√≠ pohled na nejlep≈°√≠ model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "winner_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RESULTS_PATH.exists():\n",
    "    df_results = pd.read_csv(RESULTS_PATH)\n",
    "    best_row = df_results.sort_values('test_f1', ascending=False).iloc[0]\n",
    "    \n",
    "    print(f\"üèÜ WINNER: {best_row['model']} ({best_row['scenario_name']})\")\n",
    "    print(f\"üåä Pooling: {best_row['pooling'].upper()}\")\n",
    "    print(f\"üìä F1: {best_row['test_f1']:.4f}\")\n",
    "    \n",
    "    # 1. Reload Data & Retrain\n",
    "    print(f\"üîÑ Reloading data...\")\n",
    "    data_best = data_splitting.get_train_val_test_splits(\n",
    "        scenario=best_row['scenario'],\n",
    "        level='sentence',\n",
    "        pooling=best_row['pooling'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    X_train_b, y_train_b = data_best['X_train'], data_best['y_train']\n",
    "    X_test_b, y_test_b   = data_best['X_test'], data_best['y_test']\n",
    "    \n",
    "    # Undersampling pokud je t≈ôeba (S2a)\n",
    "    if str(best_row['balance_train']) == 'True':\n",
    "        print(\"‚öñÔ∏è Applying Undersampling...\")\n",
    "        idx_0 = np.where(y_train_b == 0)[0]\n",
    "        idx_1 = np.where(y_train_b == 1)[0]\n",
    "        np.random.seed(42)\n",
    "        idx_0_sel = np.random.choice(idx_0, size=len(idx_1), replace=False)\n",
    "        idx_bal = np.concatenate([idx_0_sel, idx_1])\n",
    "        np.random.shuffle(idx_bal)\n",
    "        X_train_b, y_train_b = X_train_b[idx_bal], y_train_b[idx_bal]\n",
    "        \n",
    "    # Train\n",
    "    clf = models.get_supervised_model(best_row['model'], random_state=42)\n",
    "    clf.fit(X_train_b, y_train_b)\n",
    "    \n",
    "    # Probs\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        scores_test = clf.predict_proba(X_test_b)[:, 1]\n",
    "    else:\n",
    "        scores_test = clf.decision_function(X_test_b)\n",
    "        \n",
    "    y_pred = (scores_test > best_row['threshold']).astype(int)\n",
    "    \n",
    "    # 2. Vizualizace\n",
    "    visualization.plot_confusion_matrix_heatmap(y_test_b, y_pred, normalize=True, title=\"CM (Normalized)\")\n",
    "    visualization.plot_pr_curve(y_test_b, scores_test, title=\"PR Curve\")\n",
    "    visualization.plot_model_calibration(y_test_b, scores_test, title=\"Calibration\")\n",
    "    \n",
    "    # 3. Qualitative Analysis (Vƒõty)\n",
    "    df_qual = pd.DataFrame({\n",
    "        'text': data_best['meta_test']['text'], # Tady u≈æ m√°me cel√© vƒõty!\n",
    "        'true': y_test_b,\n",
    "        'pred': y_pred,\n",
    "        'score': scores_test\n",
    "    })\n",
    "    \n",
    "    # Kategorie\n",
    "    conds = [\n",
    "        (df_qual.true==1) & (df_qual.pred==1), (df_qual.true==0) & (df_qual.pred==0),\n",
    "        (df_qual.true==0) & (df_qual.pred==1), (df_qual.true==1) & (df_qual.pred==0)\n",
    "    ]\n",
    "    df_qual['category'] = np.select(conds, ['TP', 'TN', 'FP', 'FN'])\n",
    "    \n",
    "    print(\"\\n‚ùå TOP 5 FP (Model vid√≠ bias, kde nen√≠):\")\n",
    "    display(df_qual[df_qual['category'] == 'FP'].sort_values('score', ascending=False).head(5))\n",
    "    \n",
    "    print(\"\\n‚ùå TOP 5 FN (Model p≈ôehl√©dl bias):\")\n",
    "    display(df_qual[df_qual['category'] == 'FN'].sort_values('score', ascending=True).head(5))\n",
    "    \n",
    "    # Ulo≈æen√≠\n",
    "    df_qual.to_csv(config.RESULTS_DIR / \"M2_S2_Qualitative.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "projections",
   "metadata": {},
   "source": [
    "## 6. Projekce Embedding≈Ø\n",
    "Jak vypadaj√≠ vƒõty v prostoru? Tvo≈ô√≠ shluky?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_projections",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üé® Poƒç√≠t√°m projekce vƒõt...\")\n",
    "projs, idxs = visualization.compute_projections(X_test_b, methods=['PCA', 't-SNE'], random_state=42)\n",
    "y_viz = y_test_b[idxs]\n",
    "y_pred_viz = y_pred[idxs]\n",
    "\n",
    "for m, coords in projs.items():\n",
    "    # GT\n",
    "    visualization.plot_embedding_projection(\n",
    "        coords, pd.Series(y_viz).map({0:'Neutral', 1:'Bias'}), \n",
    "        palette={'Neutral': config.COLORS['l0'], 'Bias': config.COLORS['l1']},\n",
    "        title=f\"{m} - Ground Truth\"\n",
    "    )\n",
    "    # Errors\n",
    "    visualization.plot_error_analysis_projection(\n",
    "        coords, y_viz, y_pred_viz, method_name=m\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
