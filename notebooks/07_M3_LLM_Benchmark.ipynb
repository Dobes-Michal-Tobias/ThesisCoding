{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48153525",
   "metadata": {},
   "source": [
    "# M3: LLM Benchmark (Zero-Shot Inference)\n",
    "\n",
    "V tomto notebooku porovn√°me v√Ωkon na≈°ich natr√©novan√Ωch model≈Ø (Logistic Regression, Mahalanobis) s modern√≠mi Large Language Models (LLM) v re≈æimu Zero-Shot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e313f1",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd742794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-09 15:56:27,407 - INFO - üé® Visualization style set: whitegrid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete. Data dir: C:\\Users\\dobes\\Documents\\UniversityCodingProject\\ThesisCoding\\data\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "# Auto-reload modules for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Add src to path\n",
    "current_dir = os.getcwd()\n",
    "src_dir = os.path.abspath(os.path.join(current_dir, '..', 'src'))\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)\n",
    "\n",
    "# Import custom modules\n",
    "import config\n",
    "import data_splitting\n",
    "import models\n",
    "import visualization\n",
    "import experiments\n",
    "import evaluation\n",
    "from llm_client import LLMClassifier\n",
    "\n",
    "# Setup visualization style\n",
    "visualization.setup_style()\n",
    "\n",
    "print(f\"‚úÖ Setup complete. Data dir: {config.DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afe8c2e",
   "metadata": {},
   "source": [
    "# 1. Naƒçten√≠ Testovac√≠ch Dat\n",
    "Mus√≠me pou≈æ√≠t **stejnou testovac√≠ sadu** jako v M2/S2 (Sentence Supervised), aby bylo srovn√°n√≠ f√©r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523efd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naƒçteme Baseline sc√©n√°≈ô (S2a)\n",
    "data = data_splitting.get_train_val_test_splits(\n",
    "    scenario='baseline',\n",
    "    level='sentence',\n",
    "    pooling='mean', # Pooling tu nehraje roli, jde n√°m o text\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# N√°s zaj√≠m√° jen TEST set a p≈Øvodn√≠ texty\n",
    "df_test = data['meta_test'].copy()\n",
    "df_test['true_label'] = data['y_test']\n",
    "\n",
    "print(f\"üìä Test Set Size: {len(df_test)} sentences\")\n",
    "display(df_test.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daba6c4",
   "metadata": {},
   "source": [
    "## 2. Definice LLM Model≈Ø\n",
    "Vybereme modely, kter√© chceme testovat.\n",
    "- **Gemini:** Vy≈æaduje GOOGLE_API_KEY v .env\n",
    "- **Mistral:** Vy≈æaduje HF_TOKEN v .env (zdarma p≈ôes Inference API, ale m≈Ø≈æe m√≠t rate limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4304ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_TO_TEST = [\n",
    "    {\n",
    "        'id': 'gemini-flash',\n",
    "        'provider': 'gemini',\n",
    "        'model_name': 'gemini-1.5-flash',\n",
    "        'display_name': 'Gemini 1.5 Flash'\n",
    "    },\n",
    "    {\n",
    "        'id': 'mistral-7b',\n",
    "        'provider': 'huggingface',\n",
    "        # Pou≈æijeme v0.3, je novƒõj≈°√≠ a lep≈°√≠ v n√°sledov√°n√≠ instrukc√≠\n",
    "        'model_name': 'mistralai/Mistral-7B-Instruct-v0.3', \n",
    "        'display_name': 'Mistral 7B (HF)'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Cesta pro ukl√°d√°n√≠ v√Ωsledk≈Ø\n",
    "RESULTS_FILE = config.RESULTS_DIR / \"M3_LLM_predictions.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169c69f7",
   "metadata": {},
   "source": [
    "## 3. Inference Loop (Smyƒçka dotazov√°n√≠)\n",
    "Iterujeme p≈ôes vƒõty a modely. Pr≈Øbƒõ≈ænƒõ ukl√°d√°me v√Ωsledky, abychom o nƒõ nep≈ôi≈°li."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd675f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pokud u≈æ soubor existuje, naƒçteme ho, abychom neplatili/neƒçekali zbyteƒçnƒõ\n",
    "if RESULTS_FILE.exists():\n",
    "    df_results = pd.read_csv(RESULTS_FILE)\n",
    "    print(f\"‚ôªÔ∏è Naƒçteny p≈ôedchoz√≠ v√Ωsledky: {len(df_results)} ≈ô√°dk≈Ø.\")\n",
    "else:\n",
    "    df_results = pd.DataFrame()\n",
    "\n",
    "# Projdeme v≈°echny modely\n",
    "for model_cfg in MODELS_TO_TEST:\n",
    "    model_id = model_cfg['id']\n",
    "    print(f\"\\nüöÄ Starting Inference for: {model_cfg['display_name']}...\")\n",
    "    \n",
    "    # Inicializace klienta\n",
    "    try:\n",
    "        clf = LLMClassifier(\n",
    "            provider=model_cfg['provider'], \n",
    "            model_name=model_cfg['model_name']\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Skipper model {model_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # P≈ôiprav√≠me sloupec pro v√Ωsledky, pokud neexistuje\n",
    "    col_name = f\"pred_{model_id}\"\n",
    "    if col_name not in df_results.columns:\n",
    "        df_results[col_name] = np.nan\n",
    "        # Zkop√≠rujeme metadata z df_test, pokud je df_results pr√°zdn√Ω\n",
    "        if df_results.empty:\n",
    "            df_results = df_test.copy()\n",
    "    \n",
    "    # Smyƒçka p≈ôes vƒõty\n",
    "    # Proch√°z√≠me jen ty, kter√© je≈°tƒõ nemaj√≠ v√Ωsledek (NaN)\n",
    "    missing_mask = df_results[col_name].isna()\n",
    "    indices_to_process = df_results[missing_mask].index\n",
    "    \n",
    "    print(f\"   -> Processing {len(indices_to_process)} sentences...\")\n",
    "    \n",
    "    for idx in tqdm(indices_to_process, desc=f\"Asking {model_id}\"):\n",
    "        text = df_results.loc[idx, 'text']\n",
    "        \n",
    "        # Vol√°n√≠ API\n",
    "        prediction = clf.predict(text)\n",
    "        \n",
    "        # Ulo≈æen√≠\n",
    "        if prediction is not None:\n",
    "            df_results.loc[idx, col_name] = prediction\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Failed prediction for index {idx}\")\n",
    "        \n",
    "        # Pr≈Øbƒõ≈æn√© ulo≈æen√≠ ka≈æd√Ωch 10 vƒõt (prevence ztr√°ty dat)\n",
    "        if idx % 10 == 0:\n",
    "            df_results.to_csv(RESULTS_FILE, index=False)\n",
    "            \n",
    "    # Fin√°ln√≠ ulo≈æen√≠ po modelu\n",
    "    df_results.to_csv(RESULTS_FILE, index=False)\n",
    "    print(f\"‚úÖ Model {model_id} finished.\")\n",
    "\n",
    "print(\"\\nüèÅ All benchmarks completed.\")\n",
    "display(df_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf814b4b",
   "metadata": {},
   "source": [
    "## 4. Vyhodnocen√≠ a Srovn√°n√≠\n",
    "Spoƒç√≠t√°me metriky pro LLM a p≈ôid√°me \"ruƒçnƒõ\" nejlep≈°√≠ v√Ωsledek z M2 (LogReg), abychom mƒõli kontext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10b1055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Definice Baseline (Tv≈Øj nejlep≈°√≠ model z M2/S2)\n",
    "# Z tƒõchto zadan√Ωch csv vz√≠t nejlep≈°√≠ modely podle zadan√© matriky\n",
    "# Nap≈ô.:\n",
    "#           M2_S2_path =  config.RESULTS_DIR / \".csv\"\n",
    "\n",
    "\n",
    "\n",
    "# 2. V√Ωpoƒçet metrik pro LLM\n",
    "metrics_list = [BASELINE_PERFORMANCE]\n",
    "\n",
    "for model_cfg in MODELS_TO_TEST:\n",
    "    col_name = f\"pred_{model_cfg['id']}\"\n",
    "    \n",
    "    # Vynech√°me ≈ô√°dky, kde se API nepovedlo (NaN)\n",
    "    valid_rows = df_results.dropna(subset=[col_name])\n",
    "    \n",
    "    if len(valid_rows) == 0:\n",
    "        continue\n",
    "        \n",
    "    y_true = valid_rows['true_label']\n",
    "    y_pred = valid_rows[col_name].astype(int)\n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    metrics_list.append({\n",
    "        'Model': model_cfg['display_name'],\n",
    "        'F1 Score': f1,\n",
    "        'Accuracy': acc,\n",
    "        'Type': 'LLM (Zero-Shot)'\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nüìä Report: {model_cfg['display_name']}\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Neutral', 'Bias']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8e696d",
   "metadata": {},
   "source": [
    "## 5. Vizualizace Srovn√°n√≠\n",
    "Kdo vyhr√°l? David (LogReg) nebo Goli√°≈° (Gemini)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad434a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3efb3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
