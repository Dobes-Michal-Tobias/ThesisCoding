"""
Modul pro naƒç√≠t√°n√≠ a preprocessing dat.
Obsahuje funkce pro NLP pipeline (spaCy + RobeCzech) a generov√°n√≠ vektor≈Ø.
"""

import os
import json
import torch
import numpy as np
import pandas as pd
import spacy_udpipe
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm
from pathlib import Path
import pickle
import json

# Import konfigurace
import config

nlp = None
tokenizer = None
model = None

def initialize_models():
    """
    Inicializuje spaCy a BERT modely, pokud je≈°tƒõ nejsou naƒçteny.
    """
    global nlp, tokenizer, model
    
    if nlp is None:
        print("‚è≥ Naƒç√≠t√°m spaCy-UDPipe model ('cs-pdt')...")
        # Sta≈æen√≠ modelu, pokud nen√≠ (bezpeƒçn√© vol√°n√≠)
        try:
            spacy_udpipe.load("cs-pdt")
        except:
            spacy_udpipe.download("cs-pdt")
        nlp = spacy_udpipe.load("cs-pdt")
    
    if model is None:
        print(f"‚è≥ Naƒç√≠t√°m RobeCzech model ('{config.MODEL_NAME}')...")
        tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME, add_prefix_space=True)
        model = AutoModel.from_pretrained(config.MODEL_NAME)
        model.to(config.DEVICE)
        model.eval()
        
    print("‚úÖ Modely p≈ôipraveny.")

def get_bert_embeddings(text_list):
    """
    Z√≠sk√° embeddingy pro seznam slov pomoc√≠ BERTa.
    Vrac√≠: (aligned_embeddings, cls_token, sent_mean_embedding)
    """
    # Tokenizace pro BERT
    inputs = tokenizer(
        text_list,
        is_split_into_words=True,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=config.MAX_LENGTH
    ).to(config.DEVICE)
    
    with torch.no_grad():
        outputs = model(**inputs)
    
    # Z√≠sk√°n√≠ hidden states [batch, seq_len, hidden]
    last_hidden_states = outputs.last_hidden_state
    
    # --- A) CLS Token ---
    # Prvn√≠ token v sekvenci (index 0)
    cls_embedding = last_hidden_states[0, 0, :].cpu().numpy()
    
    # --- B) Sentence Mean Pooling (NOV√â) ---
    # Mus√≠me vz√≠t pr≈Ømƒõr v≈°ech token≈Ø, ale IGNOROVAT padding!
    attention_mask = inputs['attention_mask']
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()
    
    sum_embeddings = torch.sum(last_hidden_states * input_mask_expanded, 1)
    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    sent_mean_embedding = (sum_embeddings / sum_mask)[0].cpu().numpy()

    # --- C) Word Alignment (P≈Øvodn√≠ logika) ---
    # Zde pracujeme s [0], proto≈æe batch_size=1
    sequence_output = last_hidden_states[0].cpu()
    word_ids = inputs.word_ids()
    aligned_embeddings = []
    
    current_word_idx = None
    current_subtokens = []
    
    for i, word_id in enumerate(word_ids):
        if word_id is None: # Skip special tokens [CLS], [SEP]
            continue
            
        if word_id != current_word_idx:
            if current_subtokens:
                avg_emb = torch.stack(current_subtokens).mean(dim=0).numpy()
                aligned_embeddings.append(avg_emb)
            current_word_idx = word_id
            current_subtokens = []
        
        current_subtokens.append(sequence_output[i])
        
    if current_subtokens:
        avg_emb = torch.stack(current_subtokens).mean(dim=0).numpy()
        aligned_embeddings.append(avg_emb)
        
    # VRAC√çME T≈òI HODNOTY (Tokens, CLS, Mean)
    return aligned_embeddings, cls_embedding, sent_mean_embedding

def process_row(row):
    """
    Zpracuje jeden ≈ô√°dek datasetu.
    Vrac√≠: (final_tokens, sentence_embeddings_dict)
    """
    texts_parts = [
        (0, row.get('context_prev')), 
        (1, row.get('target_sentence')), 
        (2, row.get('context_next'))
    ]
    
    final_tokens = []
    
    # Slovn√≠k pro v≈°echny vƒõty (kl√≠ƒç bude string '0', '1', '2')
    all_sent_embs = {}
    
    for sent_id, text in texts_parts:
        if not text or pd.isna(text):
            continue
            
        # 1. SpaCy
        doc = nlp(text)
        words_for_bert = []
        spacy_tokens_info = []
        
        for token in doc:
            if token.is_space: continue
            words_for_bert.append(token.text)
            spacy_tokens_info.append({
                "form": token.text,
                "lemma": token.lemma_,
                "pos": token.pos_,
                "sent_id": sent_id
            })
            
        if not words_for_bert: continue
            
        # 2. BERT Embeddingy
        embeddings, cls_emb, mean_emb = get_bert_embeddings(words_for_bert)
        
        # 3. ZMƒöNA: Ukl√°d√°me embeddingy pro KA≈ΩDOU vƒõtu (nejen pro target)
        all_sent_embs[str(sent_id)] = {
            'cls': cls_emb.tolist(),
            'mean': mean_emb.tolist()
        }
        
        # 4. Spojen√≠ info pro slova
        for i, info in enumerate(spacy_tokens_info):
            if i < len(embeddings):
                info['embedding'] = embeddings[i].tolist()
                final_tokens.append(info)
                
    # Vrac√≠me tokens a slovn√≠k se v≈°emi vƒõtn√Ωmi vektory
    return final_tokens, all_sent_embs

def create_interim_jsonl(input_path, output_path):
    """
    Naƒçte RAW JSONL, p≈ôid√° embeddingy a ulo≈æ√≠ do Interim JSONL.
    """
    initialize_models()
    
    data = []
    with open(input_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data.append(json.loads(line))
            except:
                continue
                
    df = pd.DataFrame(data)
    print(f"üöÄ Zpracov√°v√°m {len(df)} ≈ô√°dk≈Ø z {input_path}...")
    
    # Aplikace process_row s progress barem
    tqdm.pandas(desc="NLP Pipeline")
    
    # --- ZMƒöNA ZDE ---
    # process_row nyn√≠ vrac√≠ (tokens, sent_embs_dict)
    # Pou≈æijeme pomocnou lambda funkci pro aplikaci a v√Ωsledek ulo≈æ√≠me do doƒçasn√© Series
    processed_series = df.progress_apply(process_row, axis=1)
    
    # Rozbalen√≠ v√Ωsledk≈Ø do sloupc≈Ø DataFrame
    # 1. Sloupec 'tokens' (obsahuje seznam slov s embeddingy)
    df['tokens'] = processed_series.apply(lambda x: x[0])

    # 2. ZMƒöNA: Ukl√°d√°me cel√Ω slovn√≠k vƒõtn√Ωch embedding≈Ø
    # Bude obsahovat keys "0", "1", "2" (pokud existuj√≠)
    df['sentence_vectors'] = processed_series.apply(lambda x: x[1])
    # (Star√© sloupce 'sent_cls_emb' u≈æ nepot≈ôebujeme, v≈°e je v 'sentence_vectors')
    
    # Ulo≈æen√≠
    print(f"üíæ Ukl√°d√°m Interim dataset do {output_path}...")
    df.to_json(output_path, orient='records', lines=True, force_ascii=False)
    print("‚úÖ Hotovo.")
    return df

# --- FUNKCE PRO GENEROV√ÅN√ç VEKTOROV√ùCH ARTEFAKT≈Æ ---
def _initialize_storage():
    """P≈ôiprav√≠ pr√°zdn√© kontejnery pro data."""
    store = {
        'token': {
            'none': {'l0': [], 'l1': []},
            'mild': {'l0': [], 'l1': []},
            'aggressive': {'l0': [], 'l1': []}
        },
        'sentence': {
            'mean': {'l0': [], 'l1': []},
            'cls': {'l0': [], 'l1': []}
        },
        # Augmentace pro Gold (Context)
        'gold_context': {
            'token': {'none': [], 'mild': [], 'aggressive': []},
            'sentence': {'mean': [], 'cls': []}
        }
    }
    return store

def _is_token_kept(pos_tag, filter_type):
    """Rozhodne, zda token projde filtrem."""
    if filter_type == 'none':
        return True
    if filter_type == 'mild':
        return pos_tag not in config.POS_FORBIDDEN_MILD
    if filter_type == 'aggressive':
        return pos_tag in config.POS_ALLOWED_AGGRESSIVE
    return False

def _process_sentence_level(row, store, dataset_name):
    """
    Zpracuje embeddingy cel√Ωch vƒõt.
    ƒåte 'True CLS' a 'True Mean' z p≈ôipraven√©ho slovn√≠ku sentence_vectors.
    """
    label = row['label']
    
    # Naƒçteme slovn√≠k s vektory (pokud chyb√≠, je pr√°zdn√Ω)
    sent_vecs = row.get('sentence_vectors', {})
    if not sent_vecs:
        return # Nem√°me data, konƒç√≠me

    # --- A. TARGET VƒöTA (ID '1') ---
    target_data = sent_vecs.get('1')
    
    if target_data:
        mean_emb = target_data['mean']
        cls_emb = target_data['cls']
        
        # Ukl√°d√°n√≠ (Logic: V≈°echno bereme, Gold i Silver)
        key = 'l1' if label == 1 else 'l0'
        
        # Ukl√°d√°me do store
        store['sentence']['mean'][key].append(mean_emb)
        store['sentence']['cls'][key].append(cls_emb)

    # --- B. KONTEXTOV√â VƒöTY (ID '0', '2') - Pouze pro Gold Augmentaci ---
    if dataset_name == 'gold':
        for ctx_id in ['0', '2']: # Pozor: kl√≠ƒçe v JSON jsou stringy
            ctx_data = sent_vecs.get(ctx_id)
            
            if ctx_data:
                store['gold_context']['sentence']['mean'].append(ctx_data['mean'])
                store['gold_context']['sentence']['cls'].append(ctx_data['cls'])

def _process_token_level(row, store, dataset_name):
    """Zpracuje embeddingy jednotliv√Ωch token≈Ø."""
    label = row['label']
    target_token_form = row.get('target_token')
    
    for t in row['tokens']:
        emb = t['embedding']
        pos = t['pos']
        form = t['form']
        sent_id = t['sent_id']
        
        # Identifikace role tokenu
        is_anomaly = (label == 1 and form == target_token_form and sent_id == 1)
        is_target_l0 = (label == 0 and sent_id == 1) # ƒåist√© slovo v target vƒõtƒõ
        is_context = (sent_id != 1)
        
        # Pokud je to Silver a nen√≠ to anom√°lie, token n√°s nezaj√≠m√° (Silver L0 zahazujeme)
        # if dataset_name == 'silver' and not is_anomaly: continue

        for f_type in ['none', 'mild', 'aggressive']:
            if _is_token_kept(pos, f_type):
                if is_anomaly:
                    store['token'][f_type]['l1'].append(emb)
                
                elif is_target_l0: # Odstranƒõno: and dataset_name == 'gold'
                    store['token'][f_type]['l0'].append(emb)
                
                elif is_context and dataset_name == 'gold':
                    store['gold_context']['token'][f_type].append(emb)

def _save_to_disk(store, dataset_name):
    """Ulo≈æ√≠ nasb√≠ran√° data do .pkl soubor≈Ø."""
    save_dir = config.VECTORS_DIR / dataset_name.lower()
    save_dir.mkdir(parents=True, exist_ok=True)
    print(f"üíæ Saving artifacts to {save_dir}...")

    def _dump(data, filename):
        if not data: return
        # Konverze na numpy array pro efektivitu
        arr = np.array(data, dtype=np.float32)
        with open(save_dir / filename, 'wb') as f:
            pickle.dump(arr, f)
        del arr # √öklid pamƒõti

    # 1. Ulo≈æen√≠ Token≈Ø
    for f_type, dict_l0_l1 in store['token'].items():
        _dump(dict_l0_l1['l0'], f"{dataset_name}_token_{f_type}_l0.pkl")
        _dump(dict_l0_l1['l1'], f"{dataset_name}_token_{f_type}_l1.pkl")
        
    # 2. Ulo≈æen√≠ Vƒõt
    for pool_type, dict_l0_l1 in store['sentence'].items():
        _dump(dict_l0_l1['l0'], f"{dataset_name}_sent_{pool_type}_l0.pkl")
        _dump(dict_l0_l1['l1'], f"{dataset_name}_sent_{pool_type}_l1.pkl")

    # 3. Ulo≈æen√≠ Gold Augmentace - ZMƒöNA ZDE
    if dataset_name == 'gold':
        # Nyn√≠ ukl√°d√°me oba typy vektor≈Ø zvl√°≈°≈•
        _dump(store['gold_context']['sentence']['mean'], "gold_context_sent_mean.pkl")
        _dump(store['gold_context']['sentence']['cls'], "gold_context_sent_cls.pkl")
        
        for f_type, data in store['gold_context']['token'].items():
            _dump(data, f"gold_context_token_{f_type}.pkl")
            
    print("‚úÖ Artifacts saved successfully.")

# --- HLAVN√ç FUNKCE (Public) ---

def generate_vector_artifacts(interim_path, dataset_name):
    """
    Hlavn√≠ ≈ô√≠d√≠c√≠ funkce. ƒåte soubor ≈ô√°dek po ≈ô√°dku a vol√° procesory.
    """
    print(f"\nüî® Generuji vektorov√© artefakty pro: {dataset_name.upper()}")
    
    if not os.path.exists(interim_path):
        print(f"‚ùå Error: File not found: {interim_path}")
        return

    # 1. Inicializace
    store = _initialize_storage()

    # Zji≈°tƒõn√≠ poƒçtu ≈ô√°dk≈Ø (pro progress bar)
    total_lines = sum(1 for _ in open(interim_path, 'r', encoding='utf-8'))
    
    # 2. Hlavn√≠ smyƒçka (Streaming)
    print(f"   -> Processing {total_lines} lines stream-wise...")
    
    with open(interim_path, 'r', encoding='utf-8') as f:
        for line in tqdm(f, total=total_lines, desc="Extracting Vectors"):
            try:
                row = json.loads(line)
            except json.JSONDecodeError:
                continue

            # Delegov√°n√≠ pr√°ce na pod-funkce
            _process_sentence_level(row, store, dataset_name)
            _process_token_level(row, store, dataset_name)
            
    # 3. Ulo≈æen√≠ v√Ωsledk≈Ø
    _save_to_disk(store, dataset_name)