"""
Modul pro kvalitativn√≠ anal√Ωzu model≈Ø.
Spojuje predikce modelu zpƒõt s p≈Øvodn√≠m textem (slova/vƒõty) a identifikuje chyby.
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path
import config

# --- A. NAƒå√çT√ÅN√ç TEXT≈Æ ---

def load_jsonl_texts(file_path):
    """
    Naƒçte texty z JSONL souboru.
    Vrac√≠ seznam slovn√≠k≈Ø: [{'text': '...', 'label': ...}, ...]
    """
    data = []
    path = Path(file_path)
    if not path.exists():
        print(f"‚ö†Ô∏è Soubor nenalezen: {path}")
        return []
        
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            rec = json.loads(line)
            # Ulo≈æ√≠me si to, co pot≈ôebujeme (text, lemma, label)
            # Pokud jde o token level, 'text' je pole token≈Ø. 
            # Pokud sentence level, 'text' je string.
            data.append(rec)
    return data

def get_flat_texts_and_labels(jsonl_data, level='sentence', filter_type='aggressive'):
    """
    P≈ôevede slo≈æitou JSONL strukturu na ploch√Ω seznam text≈Ø, kter√Ω odpov√≠d√° vektor≈Øm.
    """
    l0_items = []
    l1_items = []
        
    for entry in jsonl_data:
        # --- SENTENCE LEVEL (M2/S2) ---
        if level == 'sentence':
            # ZDE BYLA CHYBA: Spr√°vn√Ω kl√≠ƒç je 'target_sentence'
            txt = entry.get('target_sentence')
            lbl = entry.get('label')
            
            # Pokud by n√°hodou kl√≠ƒç chybƒõl, zkus√≠me fallback
            if txt is None:
                txt = entry.get('text') or entry.get('sentence')
            
            if txt is not None and lbl is not None:
                if lbl == 0:
                    l0_items.append(txt)
                else:
                    l1_items.append(txt)
                
        # --- TOKEN LEVEL (M2/S1) ---
        elif level == 'token':
            # Tady budeme muset b√Ωt opatrn√≠, a≈æ to spust√≠te pro S1.
            # Podle va≈°ich kl√≠ƒç≈Ø tam vid√≠m 'tokens', ale nevid√≠m 'pos_tags' nebo 'labels' (seznam pro tokeny).
            # Pokud tyto kl√≠ƒçe v JSONL chyb√≠, budeme muset logiku pro Token level upravit.
            # Zat√≠m nech√°v√°m p≈Øvodn√≠ logiku, pokud tam ty kl√≠ƒçe nƒõkde jsou:
            
            tokens = entry.get('tokens', [])
            pos_tags = entry.get('pos_tags', []) # Pokud chyb√≠, vr√°t√≠ pr√°zdn√Ω list
            labels = entry.get('labels', [])     # Pokud chyb√≠, vr√°t√≠ pr√°zdn√Ω list
            
            # Pokud nem√°me tagy/labely pro tokeny, nem≈Ø≈æeme filtrovat -> p≈ôeskoƒç√≠me
            if not tokens or not pos_tags or not labels:
                continue

            for t, pos, lbl in zip(tokens, pos_tags, labels):
                keep = False
                if filter_type == 'none':
                    keep = True
                elif filter_type == 'aggressive':
                    if pos in config.POS_ALLOWED_AGGRESSIVE: keep = True
                elif filter_type == 'mild':
                    if pos not in config.POS_FORBIDDEN_MILD: keep = True
                
                if keep:
                    if lbl == 0:
                        l0_items.append(t)
                    else:
                        l1_items.append(t)
                        
    return np.array(l0_items), np.array(l1_items)

# --- B. TVORBA ANALYTICK√âHO DATAFRAME ---

def create_analysis_df(model, X_test, y_test, texts_test):
    """
    Vytvo≈ô√≠ DataFrame spojuj√≠c√≠ text, ground truth, predikci a pravdƒõpodobnost.
    
    Args:
        model: Natr√©novan√Ω model (mus√≠ m√≠t predict_proba nebo decision_function)
        X_test: Testovac√≠ vektory
        y_test: Testovac√≠ labely
        texts_test: Testovac√≠ texty (mus√≠ m√≠t stejnou d√©lku jako X_test!)
    """
    # 1. Z√≠sk√°n√≠ pravdƒõpodobnost√≠
    if hasattr(model, "predict_proba"):
        probs = model.predict_proba(X_test)[:, 1]
    else:
        # Fallback pro SVM/Linear
        d = model.decision_function(X_test)
        probs = (d - d.min()) / (d.max() - d.min()) # Normalizace 0-1
        
    # 2. Predikce
    preds = model.predict(X_test)
    
    # 3. Tvorba DF
    df = pd.DataFrame({
        'Text': texts_test,
        'True_Label': y_test,
        'Pred_Label': preds,
        'Prob_Anomaly': probs
    })
    
    # 4. Kategorizace chyby
    def get_category(row):
        if row['True_Label'] == 1 and row['Pred_Label'] == 1: return 'TP'
        if row['True_Label'] == 0 and row['Pred_Label'] == 0: return 'TN'
        if row['True_Label'] == 0 and row['Pred_Label'] == 1: return 'FP'
        if row['True_Label'] == 1 and row['Pred_Label'] == 0: return 'FN'
        return 'Unknown'

    df['Error_Type'] = df.apply(get_category, axis=1)
    
    return df

# --- C. REPORTING FUNKCE ---

def show_top_anomalies(df, n=15):
    """Uk√°≈æe texty, kter√© model s nejvy≈°≈°√≠ jistotou oznaƒçil za anom√°lie."""
    print(f"\nüî• TOP {n} DETEKOVAN√ùCH ANOM√ÅLI√ç (Nejvy≈°≈°√≠ jistota modelu):")
    # Filtrujeme jen ty, co model oznaƒçil jako 1 (Anomaly)
    subset = df[df['Pred_Label'] == 1].sort_values(by='Prob_Anomaly', ascending=False).head(n)
    
    # Form√°tovan√Ω v√Ωpis
    print(f"{'Prob':<8} | {'Type':<4} | {'Text'}")
    print("-" * 60)
    for _, row in subset.iterrows():
        # Zv√Ωraznƒõn√≠ chyby (FP) hvƒõzdiƒçkou
        mark = "‚ö†Ô∏è" if row['Error_Type'] == 'FP' else "‚úÖ"
        print(f"{row['Prob_Anomaly']:.4f} | {row['Error_Type']:<4} | {row['Text']} {mark}")

def show_worst_errors(df, error_type='FP', n=15):
    """
    Uk√°≈æe nejhor≈°√≠ chyby.
    FP (False Positive): Neutr√°ln√≠ texty, kde si byl model nejjistƒõj≈°√≠, ≈æe jsou anom√°lie.
    FN (False Negative): Anom√°ln√≠ texty, kter√© model s jistotou oznaƒçil za neutr√°ln√≠.
    """
    print(f"\n‚ùå TOP {n} CHYB TYPU {error_type} (Kde se model nejv√≠c spletl):")
    
    if error_type == 'FP':
        # Chceme FP s nejvy≈°≈°√≠m Prob_Anomaly (model si myslel, ≈æe je to anom√°lie)
        subset = df[df['Error_Type'] == 'FP'].sort_values(by='Prob_Anomaly', ascending=False).head(n)
    else:
        # Chceme FN s nejni≈æ≈°√≠m Prob_Anomaly (model si myslel, ≈æe je to urƒçitƒõ Neutral)
        subset = df[df['Error_Type'] == 'FN'].sort_values(by='Prob_Anomaly', ascending=True).head(n)
        
    if subset.empty:
        print("   (≈Ω√°dn√© chyby tohoto typu nenalezeny. Skvƒõl√° pr√°ce modelu!)")
        return

    print(f"{'Prob':<8} | {'Text'}")
    print("-" * 60)
    for _, row in subset.iterrows():
        print(f"{row['Prob_Anomaly']:.4f} | {row['Text']}")