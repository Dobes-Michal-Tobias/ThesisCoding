"""
Modul pro kvalitativn√≠ anal√Ωzu model≈Ø.
Slou≈æ√≠ k propojen√≠ vektorov√Ωch predikc√≠ s p≈Øvodn√≠m textem.
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path
from sklearn.model_selection import train_test_split
import config

# --- POMOCN√â FUNKCE PRO NAƒå√çT√ÅN√ç ---

def load_jsonl_texts(file_path):
    """Naƒçte surov√° JSONL data."""
    data = []
    path = Path(file_path)
    if not path.exists():
        print(f"Soubor nenalezen: {path}")
        return []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data.append(json.loads(line))
            except json.JSONDecodeError:
                continue
    return data

def _is_token_kept(pos, filter_type):
    """Kopie logiky filtrace z preprocessingu (mus√≠ b√Ωt 1:1)."""
    if filter_type == 'none': return True
    # Definice sad tag≈Ø
    
    if filter_type == 'aggressive':
        return pos in config.POS_ALLOWED_AGGRESSIVE
    if filter_type == 'mild':
        return pos not in config.POS_FORBIDDEN_MILD
    return False

# --- HLAVN√ç EXTRAKƒåN√ç LOGIKA ---

def extract_aligned_metadata(raw_data, level='sentence', scenario='baseline', filter_type='aggressive'):
    """
    Projdede raw data a extrahuje metadata (texty, tokeny, labely)
    P≈òESNƒö v tom po≈ôad√≠, v jak√©m byly generov√°ny vektory.
    
    Returns:
        list of dict: [{'text': ..., 'token': ..., 'label': ...}, ...]
    """
    metadata_list = []
    
    # 1. SENTENCE LEVEL LOGIKA
    if level == 'sentence':
        # Proch√°z√≠me data
        for entry in raw_data:
            s_vecs = entry.get('sentence_vectors', {})
            
            # A) Target Sentence (ID '1') - V≈ædy p≈ô√≠tomna v datech
            if '1' in s_vecs:
                txt = entry.get('target_sentence') or entry.get('text')
                lbl = entry.get('label')
                if txt is not None:
                    metadata_list.append({
                        'text': txt,
                        'analyzed_token': None, # U vƒõt ned√°v√° smysl
                        'label': lbl,
                        'origin': 'target'
                    })

            # B) Context Sentences (ID '0', '2') - Pouze pro ROBUSTNESS a jen z GOLD dat
            # Pozor: Logika mus√≠ odpov√≠dat tomu, jak jsme skl√°dali X_train/X_test
            # Pokud vol√°me tuto funkci pro Silver data, kontext obvykle ignorujeme (v load_data_s2 se context bere jen z Gold)
            if scenario == 'robustness' and 'label' in entry and entry['label'] is not None: 
                 # Jednoduch√° heuristika: Context bereme jen pokud je zdroj 'gold' (co≈æ pozn√°me podle entry, nebo to vy≈ôe≈°√≠me vnƒõ)
                 # Zde pro jistotu extrahujeme v≈°e, filtrov√°n√≠ probƒõhne p≈ôi skl√°d√°n√≠ datasetu
                 pass 

    # 2. TOKEN LEVEL LOGIKA
    elif level == 'token':
        for entry in raw_data:
            tokens = entry.get('tokens', [])
            # Pokud nem√°me tokeny s pos_tags, nem≈Ø≈æeme filtrovat
            if not tokens: continue
            
            # Mus√≠me iterovat p≈ôes tokeny a aplikovat filtr
            for t in tokens:
                pos = t.get('pos')
                form = t.get('form')
                sent_id = t.get('sent_id') # 0, 1, 2
                emb = t.get('embedding') # Jen kontrola, ≈æe existuje
                
                # Filtr: Bereme jen tokeny, kter√© maj√≠ embedding a projdou POS filtrem
                if emb is None: continue
                
                # Logika v√Ωbƒõru token≈Ø (mus√≠ sedƒõt s load_preprocess_data.py -> _process_token_level)
                # V M2/S1 bereme anom√°lie (L1) a Target L0 (L0). Context tokeny (sent_id != 1) se braly jen do 'gold_context'
                
                # Zjednodu≈°en√≠: Vrac√≠me v≈°echno, co projde POS filtrem. 
                # N√°sledn√° selekce (L0 vs L1) probƒõhne vnƒõ funkce.
                if _is_token_kept(pos, filter_type):
                    
                    # Urƒçen√≠ labelu pro dan√Ω token
                    # V datasetu je label pro celou vƒõtu.
                    # Token je anom√°lie jen pokud (label_vƒõty==1 AND token==target_token AND sent_id==1)
                    row_label = entry.get('label')
                    target_token_str = entry.get('target_token')
                    
                    is_anomaly = (row_label == 1 and form == target_token_str and sent_id == 1)
                    token_label = 1 if is_anomaly else 0
                    
                    # Ukl√°d√°me
                    metadata_list.append({
                        'text': entry.get('target_sentence'), # Kontext (cel√° vƒõta)
                        'analyzed_token': form,               # Co analyzujeme
                        'label': token_label,
                        'sent_id': sent_id
                    })
                    
    return metadata_list

# --- FUNKCE PRO GENEROV√ÅN√ç V√ùSLEDK≈Æ (PUBLIC) ---

def generate_predictions_csv(model, X_test, y_test, model_name, scenario, level='sentence', filter_type='aggressive'):
    """
    Kompletn√≠ pipeline s automatickou detekc√≠ chybƒõj√≠c√≠ch Silver dat (Fallback S1e -> S1b).
    """
    print(f"üîÑ Generuji CSV report pro: {model_name} ({scenario}, {level})")
    
    # 1. Naƒçten√≠ Raw Dat
    path_gold = config.INTERIM_DIR / 'GOLD_data_processed.jsonl'
    path_silver = config.INTERIM_DIR / 'SILVER_data_processed.jsonl'
    
    raw_gold = load_jsonl_texts(path_gold)
    raw_silver = load_jsonl_texts(path_silver)
    
    meta_final = [] 
    
    # ---------------------------------------------------------
    # A) REKONSTRUKCE DAT
    # ---------------------------------------------------------
    
    if level == 'sentence':
        # ... (Logika pro vƒõty - beze zmƒõny) ...
        g_meta = extract_aligned_metadata(raw_gold, level='sentence')
        s_meta = extract_aligned_metadata(raw_silver, level='sentence')
        
        g_l0 = [m for m in g_meta if m['label'] == 0]
        g_l1 = [m for m in g_meta if m['label'] == 1]
        s_l1 = [m for m in s_meta if m['label'] == 1]
        
        g_context = []
        if scenario == 'robustness':
            for entry in raw_gold:
                s_vecs = entry.get('sentence_vectors', {})
                if '0' in s_vecs: g_context.append({'text': entry.get('context_prev'), 'label': 0, 'origin': 'context'})
                if '2' in s_vecs: g_context.append({'text': entry.get('context_next'), 'label': 0, 'origin': 'context'})
        
        if scenario == 'baseline':
            meta_final = g_l0 + g_l1
        elif scenario == 'robustness':
            meta_final = g_l0 + g_context + g_l1 + s_l1
            
    elif level == 'token':
        # -- Token Level Logic --
        g_meta = extract_aligned_metadata(raw_gold, level='token', filter_type=filter_type)
        s_meta = extract_aligned_metadata(raw_silver, level='token', filter_type=filter_type)
        
        # 1. Rozdƒõlen√≠
        g_l0 = [m for m in g_meta if m['label'] == 0 and m['sent_id'] == 1]
        g_l1 = [m for m in g_meta if m['label'] == 1]
        s_l1 = [m for m in s_meta if m['label'] == 1]
        
        # 2. Logika skl√°d√°n√≠
        if scenario == 'S1a': 
            meta_final = g_l0 + g_l1
            
        elif scenario == 'S1b':
            n_anomalies = len(g_l1)
            if len(g_l0) > n_anomalies:
                np.random.seed(42)
                idx = np.random.choice(len(g_l0), n_anomalies, replace=False)
                g_l0_selected = [g_l0[i] for i in idx]
                meta_final = g_l0_selected + g_l1
            else:
                meta_final = g_l0 + g_l1

        elif scenario == 'S1d':
            # Train Noisy -> Test Clean (pouze Gold)
            meta_final = g_l0 + g_l1
            
        elif scenario == 'S1e' or 'S1' in scenario:
            # Hybrid (Gold + Silver L1)
            
            # --- DETEKCE CHYBƒöJ√çC√çCH SILVER VEKTOR≈Æ ---
            # Vypoƒç√≠t√°me oƒçek√°vanou velikost testovac√≠ sady, kdybychom mƒõli i Silver
            total_full = (len(g_l1) + len(s_l1)) * 2 # Anom√°lie + Balance L0
            expected_test_full = int(total_full * 0.2)
            
            # Pokud je X_test v√Ωraznƒõ men≈°√≠, ne≈æ ƒçek√°me se Silver daty,
            # znamen√° to, ≈æe se model natr√©noval bez nich (Fallback na Gold Only).
            is_silver_missing = len(X_test) < (expected_test_full * 0.8) # 20% tolerance
            
            if is_silver_missing:
                print(f"‚ö†Ô∏è VAROV√ÅN√ç: Detekov√°n nesoulad dat!")
                print(f"   Oƒçek√°v√°no cca {expected_test_full} testovac√≠ch vzork≈Ø (se Silver), ale m√°me jen {len(X_test)}.")
                print(f"   -> Model byl pravdƒõpodobnƒõ tr√©nov√°n BEZ Silver dat (chyb√≠ vektory?).")
                print(f"   -> P≈ôep√≠n√°m anal√Ωzu na 'Gold Only' re≈æim (jako S1b), aby to pro≈°lo.")
                
                # Fallback: Pou≈æijeme jen Gold anom√°lie
                all_l1 = g_l1 
            else:
                all_l1 = g_l1 + s_l1

            # --- V√ùBƒöR L0 (UNDERSAMPLING) ---
            # Mus√≠ odpov√≠dat poƒçtu anom√°li√≠ (a≈• u≈æ s nebo bez Silver)
            n_anomalies = len(all_l1)
            
            if len(g_l0) >= n_anomalies:
                np.random.seed(42) # D≈ÆLE≈ΩIT√â: Mus√≠ sedƒõt s tr√©ninkem
                idx = np.random.choice(len(g_l0), n_anomalies, replace=False)
                g_l0_selected = [g_l0[i] for i in idx]
            else:
                g_l0_selected = g_l0

            meta_final = g_l0_selected + all_l1

    # ---------------------------------------------------------
    # B) SPLIT
    # ---------------------------------------------------------
    if not meta_final:
        return None

    labels_all = [m['label'] for m in meta_final]
    
    # Split
    _, meta_test, _, _ = train_test_split(
        meta_final, labels_all, 
        test_size=0.2, 
        stratify=labels_all, 
        random_state=42
    )
    
    # KONTROLA POƒåT≈Æ
    if len(meta_test) != len(X_test):
        print(f"‚ùå KRIITICK√Å CHYBA: Ani po korekci poƒçty nesed√≠.")
        print(f"   Metadata: {len(meta_test)}, Vektory: {len(X_test)}")
        return None

    # ... (Zbytek beze zmƒõny) ...
    
    # C) PREDIKCE
    if hasattr(model, "predict_proba"):
        probs = model.predict_proba(X_test)[:, 1]
    else:
        d = model.decision_function(X_test)
        probs = (d - d.min()) / (d.max() - d.min())

    preds = model.predict(X_test)
    
    df = pd.DataFrame(meta_test)
    df['prob_anomaly'] = probs
    df['pred_label'] = preds
    df['true_label'] = y_test
    
    conditions = [
        (df['true_label'] == 1) & (df['pred_label'] == 1),
        (df['true_label'] == 0) & (df['pred_label'] == 0),
        (df['true_label'] == 0) & (df['pred_label'] == 1),
        (df['true_label'] == 1) & (df['pred_label'] == 0)
    ]
    df['error_category'] = np.select(conditions, ['TP', 'TN', 'FP', 'FN'], default='Unknown')
    df['model'] = model_name
    df['scenario'] = scenario
    
    cols = ['text', 'analyzed_token', 'true_label', 'pred_label', 'prob_anomaly', 'error_category', 'model', 'scenario']
    cols = [c for c in cols if c in df.columns]
    
    return df[cols]